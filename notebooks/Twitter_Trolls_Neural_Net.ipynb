{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhimanyu\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# import stuff\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from random import randint\n",
    "import h5py\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import tweepy\n",
    "import sys\n",
    "import jsonpickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Pytorch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD\n",
    "from torch.autograd import Variable\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'C:\\\\Users\\\\Abhimanyu\\\\Documents\\\\Coding\\\\Twitter_AC209a\\\\group\\\\troll_classification')\n",
    "# print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size : 10000\n"
     ]
    }
   ],
   "source": [
    "# InferSent setup\n",
    "from InferSent.models import *\n",
    "model_version = 1\n",
    "MODEL_PATH = \"../InferSent/encoder/infersent%s.pkl\" % model_version\n",
    "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': model_version}\n",
    "model = InferSent(params_model)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "# Keep it on CPU or put it on GPU\n",
    "use_cuda = False\n",
    "model = model.cuda() if use_cuda else model\n",
    "\n",
    "# If infersent1 -> use GloVe embeddings. If infersent2 -> use InferSent embeddings.\n",
    "W2V_PATH = '../InferSent/dataset/GloVe/glove.840B.300d.txt' if model_version == 1 else '../dataset/fastText/crawl-300d-2M.vec'\n",
    "model.set_w2v_path(W2V_PATH)\n",
    "\n",
    "# Load embeddings of K most frequent words\n",
    "model.build_vocab_k_words(K=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetEmbeddings(sentences, batch_size=128, verbose=False):\n",
    "    embeddings = model.encode(sentences, bsize=batch_size, tokenize=False, verbose=verbose)\n",
    "    if verbose: print('nb sentences encoded : {0}'.format(len(embeddings)))\n",
    "    return embeddings\n",
    "\n",
    "def GetBatch(df, feature_cols, response,  batch_size=128):\n",
    "    '''\n",
    "    Returns a batch of:\n",
    "    (1) feature_cols\n",
    "    (2) the 'content' column which contains text of the tweet\n",
    "    (3) the response column\n",
    "    '''\n",
    "    \n",
    "    df_size = len(df)\n",
    "    for counter in range(int(len(df)/batch_size)+1):\n",
    "        yield df[feature_cols].iloc[counter*batch_size:min((counter + 1)*batch_size, len(df))], \\\n",
    "        df['content'].iloc[counter*batch_size:min((counter + 1)*batch_size, len(df))], \\\n",
    "        df[response].iloc[counter*batch_size:min((counter + 1)*batch_size, len(df))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = '../data/merged_troll_data.json'\n",
    "data_df = pd.read_json(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>followers</th>\n",
       "      <th>following</th>\n",
       "      <th>retweet</th>\n",
       "      <th>account_category</th>\n",
       "      <th>created_at</th>\n",
       "      <th>troll</th>\n",
       "      <th>orig_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>332504</td>\n",
       "      <td>3.325040e+05</td>\n",
       "      <td>332504.000000</td>\n",
       "      <td>332504.000000</td>\n",
       "      <td>332504</td>\n",
       "      <td>332504</td>\n",
       "      <td>332504</td>\n",
       "      <td>332504.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>264783</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>206846</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>RT @realDonaldTrump: Here is my statement. htt...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NonTroll</td>\n",
       "      <td>2016-10-07 07:48:00</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>298</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>166252</td>\n",
       "      <td>85</td>\n",
       "      <td>166252</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-07-01 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-11-10 18:35:32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7.935872e+03</td>\n",
       "      <td>3008.364465</td>\n",
       "      <td>0.740890</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>177103.724776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.066285e+05</td>\n",
       "      <td>7711.247498</td>\n",
       "      <td>0.438147</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>165772.625588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>6.120000e+02</td>\n",
       "      <td>541.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42472.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.144000e+03</td>\n",
       "      <td>1135.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>117701.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.431000e+03</td>\n",
       "      <td>2293.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>292016.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.086668e+07</td>\n",
       "      <td>680956.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>651703.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  content     followers  \\\n",
       "count                                              332504  3.325040e+05   \n",
       "unique                                             264783           NaN   \n",
       "top     RT @realDonaldTrump: Here is my statement. htt...           NaN   \n",
       "freq                                                  298           NaN   \n",
       "first                                                 NaN           NaN   \n",
       "last                                                  NaN           NaN   \n",
       "mean                                                  NaN  7.935872e+03   \n",
       "std                                                   NaN  2.066285e+05   \n",
       "min                                                   NaN  0.000000e+00   \n",
       "25%                                                   NaN  6.120000e+02   \n",
       "50%                                                   NaN  1.144000e+03   \n",
       "75%                                                   NaN  2.431000e+03   \n",
       "max                                                   NaN  4.086668e+07   \n",
       "\n",
       "            following        retweet account_category           created_at  \\\n",
       "count   332504.000000  332504.000000           332504               332504   \n",
       "unique            NaN            NaN                3               206846   \n",
       "top               NaN            NaN         NonTroll  2016-10-07 07:48:00   \n",
       "freq              NaN            NaN           166252                   85   \n",
       "first             NaN            NaN              NaN  2016-07-01 00:00:00   \n",
       "last              NaN            NaN              NaN  2016-11-10 18:35:32   \n",
       "mean      3008.364465       0.740890              NaN                  NaN   \n",
       "std       7711.247498       0.438147              NaN                  NaN   \n",
       "min          0.000000       0.000000              NaN                  NaN   \n",
       "25%        541.000000       0.000000              NaN                  NaN   \n",
       "50%       1135.000000       1.000000              NaN                  NaN   \n",
       "75%       2293.000000       1.000000              NaN                  NaN   \n",
       "max     680956.000000       1.000000              NaN                  NaN   \n",
       "\n",
       "         troll     orig_index  \n",
       "count   332504  332504.000000  \n",
       "unique       2            NaN  \n",
       "top       True            NaN  \n",
       "freq    166252            NaN  \n",
       "first      NaN            NaN  \n",
       "last       NaN            NaN  \n",
       "mean       NaN  177103.724776  \n",
       "std        NaN  165772.625588  \n",
       "min        NaN       0.000000  \n",
       "25%        NaN   42472.000000  \n",
       "50%        NaN  117701.500000  \n",
       "75%        NaN  292016.750000  \n",
       "max        NaN  651703.000000  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Describe of whole dataframe\n",
    "data_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['troll'] = data_df['troll'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values for column Troll: [1 0]\n",
      "Unique values for column Retweet: [1 0]\n",
      "Unique values for column Acccount Category: ['LeftTroll' 'RightTroll' 'NonTroll']\n"
     ]
    }
   ],
   "source": [
    "# Add dummy columns for categorical variables\n",
    "print('Unique values for column Troll:', data_df.troll.unique())\n",
    "print('Unique values for column Retweet:', data_df.retweet.unique())\n",
    "print('Unique values for column Acccount Category:', data_df.account_category.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['content',\n",
       " 'followers',\n",
       " 'following',\n",
       " 'retweet',\n",
       " 'created_at',\n",
       " 'troll',\n",
       " 'orig_index',\n",
       " 'Orig_account_category',\n",
       " 'account_category_LeftTroll',\n",
       " 'account_category_NonTroll',\n",
       " 'account_category_RightTroll']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_cols = ['account_category']\n",
    "for col in dummy_cols:\n",
    "    data_df['Orig_' + col] = data_df[col]\n",
    "    data_df = pd.get_dummies(data_df, columns=[col])\n",
    "list(data_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set aside columns to be used as features\n",
    "feature_cols = [\n",
    " 'followers',\n",
    " 'following',\n",
    " 'retweet',\n",
    " 'account_category_LeftTroll',\n",
    " 'account_category_NonTroll',\n",
    " 'account_category_RightTroll']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>random</th>\n",
       "      <th>temporal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>[39006, 13901, 54474, 53049, 47299, 59510, 236...</td>\n",
       "      <td>[88515, 145011, 308314, 163777, 165182, 165183...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>[115587, 272344, 110764, 17462, 161923, 109189...</td>\n",
       "      <td>[54215, 140798, 157319, 47942, 140799, 157320,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>[282180, 316618, 291427, 234801, 273642, 66628...</td>\n",
       "      <td>[286589, 289599, 296694, 287370, 150894, 28757...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  random  \\\n",
       "test   [39006, 13901, 54474, 53049, 47299, 59510, 236...   \n",
       "train  [115587, 272344, 110764, 17462, 161923, 109189...   \n",
       "val    [282180, 316618, 291427, 234801, 273642, 66628...   \n",
       "\n",
       "                                                temporal  \n",
       "test   [88515, 145011, 308314, 163777, 165182, 165183...  \n",
       "train  [54215, 140798, 157319, 47942, 140799, 157320,...  \n",
       "val    [286589, 289599, 296694, 287370, 150894, 28757...  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the indices file\n",
    "index_file = '../data/train_test_inds.json'\n",
    "idx_df = pd.read_json(index_file)\n",
    "idx_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrepareDataSplits(mode='random', batch_size=128):\n",
    "    TEST_NUM = 0\n",
    "    TRAIN_NUM = 1\n",
    "    VAL_NUM = 2\n",
    "    all_idx = list(idx_df[mode])\n",
    "    sets = ['test', 'train', 'val']\n",
    "    \n",
    "    # Standardize continuous columns based on train set statistics\n",
    "    all_dfs = {}\n",
    "    all_dfs['train'] = data_df.iloc[all_idx[TRAIN_NUM][0:400]]\n",
    "    all_dfs['val'] = data_df.iloc[all_idx[VAL_NUM][0:400]]\n",
    "    all_dfs['test'] = data_df.iloc[all_idx[TEST_NUM][0:400]]\n",
    "    \n",
    "    cols_to_standardize = ['followers', 'following']\n",
    "    for col in cols_to_standardize:\n",
    "        train_mean = all_dfs['train'][col].mean()\n",
    "        train_std = all_dfs['train'][col].std()\n",
    "        for set_type in sets:\n",
    "            all_dfs[set_type][col] = (all_dfs[set_type][col] - train_mean)/train_std\n",
    "    \n",
    "    # Compute embeddings, concatenate with other features, and write to h5py files\n",
    "    feature_length = 4096 + len(feature_cols)\n",
    "    for idx, set_type in enumerate(sets):\n",
    "        embed_file = '../data/' + set_type + '_embeddings_mode_' + mode + '.h5py'\n",
    "        num_tweets = len(all_idx[idx])\n",
    "        pbar = tqdm(total=num_tweets)\n",
    "        with h5py.File(embed_file, \"a\") as f:\n",
    "            embed_dset = f.create_dataset('features', (num_tweets, feature_length))\n",
    "            response_dset = f.create_dataset('is_troll', (num_tweets,))\n",
    "            counter = 0\n",
    "            for batch in GetBatch(all_dfs[set_type], feature_cols, 'troll', batch_size=batch_size):\n",
    "                other_features = batch[0].values\n",
    "                embeddings_  = GetEmbeddings(batch[1].values, batch_size=batch_size)\n",
    "                is_troll_ = batch[2].values\n",
    "                batch_len = other_features.shape[0]\n",
    "                feature_vec = np.hstack((embeddings_, other_features))\n",
    "                embed_dset[counter:(counter + batch_len)] = feature_vec\n",
    "                response_dset[counter:(counter + batch_len)] = is_troll_\n",
    "                counter += batch_len\n",
    "                pbar.update(batch_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhimanyu\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6652d8323b341a1a63b1a9ba1efdce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=33251), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd75caeea7c4c1698949467d7e476d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=266003), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f28dce855f514c1297465b489270a85f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=33250), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mode = 'random'\n",
    "batch_size = 512\n",
    "PrepareDataSplits(mode=mode, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read h5py file\n",
    "def GetArrays(mode, set_type):\n",
    "    embed_file = '../data/' + set_type + '_embeddings_mode_' + mode + '.h5py'\n",
    "    with h5py.File(embed_file, \"r\") as f:\n",
    "        keys = list(f.keys())\n",
    "        X = np.array(f[keys[0]])\n",
    "        y = np.array(f[keys[1]])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-106-3c896f357a42>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGetArrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGetArrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'val'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-105-3a9faa382b11>\u001b[0m in \u001b[0;36mGetArrays\u001b[1;34m(mode, set_type)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membed_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mkeys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train, y_train = GetArrays(mode, 'train')\n",
    "X_val, y_val = GetArrays(mode, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ShuffleArrays(X, y):\n",
    "    arr = np.arange(X.shape[0])\n",
    "    np.random.shuffle(arr)\n",
    "    X = X[arr,:]\n",
    "    y = y[arr]\n",
    "    return X, y\n",
    "def GetTrainBatches(X, y, batch_size=256):\n",
    "    data_len = X.shape[0]\n",
    "    for counter in range(int(data_len/batch_size)+1):\n",
    "        yield X[counter*batch_size:min((counter + 1)*batch_size, data_len), :], \\\n",
    "        y[counter*batch_size:min((counter + 1)*batch_size, data_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(TwitterNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(4102,1024)\n",
    "        self.drop1 = nn.Dropout()\n",
    "        self.fc2 = nn.Linear(1024,512)\n",
    "        self.drop2 = nn.Dropout()\n",
    "        self.fc3 = nn.Linear(512,256)\n",
    "        self.fc4 = nn.Linear(256,128)\n",
    "        self.fc5 = nn.Linear(128,num_classes)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drop2(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = Variable(torch.FloatTensor(X_train))\n",
    "y_train = Variable(torch.FloatTensor(y_train))\n",
    "X_test = Variable(torch.FloatTensor(X_test))\n",
    "y_test = Variable(torch.FloatTensor(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "model_net = TwitterNet(2).to(device)\n",
    "optimizer = SGD(model_net.parameters(), lr = 0.1, momentum=0.9)\n",
    "num_epochs = 15\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy on untrained network (with randomly initialized weights)\n",
    "torch.set_grad_enabled(False)\n",
    "running_corrects = 0\n",
    "for inputs, labels in GetTrainBatches(X_test, y_test, batch_size):\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs = model_net(inputs)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    running_corrects += torch.sum(preds == labels.long().data)\n",
    "val_acc = running_corrects.double()/X_test.shape[0]\n",
    "torch.set_grad_enabled(True)\n",
    "print('Validation Accuracy on untrained net is {:.2%}'.format(val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(True)\n",
    "for epoch in range(num_epochs):\n",
    "    X_train, y_train = ShuffleArrays(X_train, y_train)\n",
    "    running_loss = 0.0\n",
    "    train_corrects = 0\n",
    "    for inputs, labels in GetTrainBatches(X_train, y_train, batch_size):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model_net(inputs)\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        train_corrects += torch.sum(preds == labels.long().data)\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Validation set accuracy\n",
    "    train_acc = train_corrects.double()/X_train.shape[0]\n",
    "    torch.set_grad_enabled(False)\n",
    "    running_corrects = 0\n",
    "    for inputs, labels in GetTrainBatches(X_test, y_test, batch_size):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model_net(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_corrects += torch.sum(preds == labels.long().data)\n",
    "    val_acc = running_corrects.double()/X_test.shape[0]\n",
    "    torch.set_grad_enabled(True)\n",
    "    print('Loss after epoch {} is {:.3f}. Train Acc. is {:.2%} and Validation Acc. is {:.2%}'.\\\n",
    "          format(epoch+1, running_loss, train_acc, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy on trained network\n",
    "torch.set_grad_enabled(False)\n",
    "running_corrects = 0\n",
    "for inputs, labels in GetTrainBatches(X_test, y_test, batch_size):\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs = model_net(inputs)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    running_corrects += torch.sum(preds == labels.long().data)\n",
    "val_acc = running_corrects.double()/X_test.shape[0]\n",
    "torch.set_grad_enabled(True)\n",
    "print('Validation Accuracy on untrained net is {:.2%}'.format(val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y_test[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
