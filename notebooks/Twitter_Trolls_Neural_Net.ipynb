{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# import stuff\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from random import randint\n",
    "import h5py\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import tweepy\n",
    "import sys\n",
    "import jsonpickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Pytorch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD\n",
    "from torch.autograd import Variable\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size : 100000\n"
     ]
    }
   ],
   "source": [
    "# InferSent setup\n",
    "from models import InferSent\n",
    "model_version = 1\n",
    "MODEL_PATH = \"../encoder/infersent%s.pkl\" % model_version\n",
    "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': model_version}\n",
    "model = InferSent(params_model)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "# Keep it on CPU or put it on GPU\n",
    "use_cuda = False\n",
    "model = model.cuda() if use_cuda else model\n",
    "\n",
    "# If infersent1 -> use GloVe embeddings. If infersent2 -> use InferSent embeddings.\n",
    "W2V_PATH = '../dataset/GloVe/glove.840B.300d.txt' if model_version == 1 else '../dataset/fastText/crawl-300d-2M.vec'\n",
    "model.set_w2v_path(W2V_PATH)\n",
    "\n",
    "# Load embeddings of K most frequent words\n",
    "model.build_vocab_k_words(K=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetEmbeddings(sentences, batch_size=128, verbose=False):\n",
    "    embeddings = model.encode(sentences, bsize=batch_size, tokenize=False, verbose=verbose)\n",
    "    if verbose: print('nb sentences encoded : {0}'.format(len(embeddings)))\n",
    "    return embeddings\n",
    "\n",
    "def GetBatch(df, X, y, batch_size=128):\n",
    "    df_size = len(df)\n",
    "    for counter in range(int(len(df)/batch_size)+1):\n",
    "        yield df[X].iloc[counter*batch_size:min((counter + 1)*batch_size, len(df))], \\\n",
    "        df[y].iloc[counter*batch_size:min((counter + 1)*batch_size, len(df))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Non-Troll Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_troll_file = './data/non_trolls.csv'\n",
    "troll_file = './data/trolls.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhimanyu\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (0,1,3,4,7,8,9,10,11,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "non_troll_df = pd.read_csv(non_troll_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 languages used by non-trolls:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lang</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>1044537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>und</th>\n",
       "      <td>39316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>es</th>\n",
       "      <td>27108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr</th>\n",
       "      <td>6544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt</th>\n",
       "      <td>2868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>2427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>de</th>\n",
       "      <td>2198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>1332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr</th>\n",
       "      <td>1015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nl</th>\n",
       "      <td>971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Count\n",
       "lang         \n",
       "en    1044537\n",
       "und     39316\n",
       "es      27108\n",
       "fr       6544\n",
       "pt       2868\n",
       "it       2427\n",
       "de       2198\n",
       "in       1332\n",
       "tr       1015\n",
       "nl        971"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Top 10 languages used by non-trolls:')\n",
    "non_troll_df.groupby('lang').agg({'id' : 'count'}).sort_values(by='id', ascending=False).rename(columns={'id' : 'Count'}).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1044537"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_troll_en_df = non_troll_df.loc[non_troll_df.lang == 'en']\n",
    "len(non_troll_en_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 20000\n",
    "non_troll_sample_df = non_troll_en_df.sample(num_samples)\n",
    "non_troll_sample_df['is_troll'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_troll_text_df = non_troll_sample_df[['text', 'is_troll']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Troll Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "troll_df = pd.read_csv('./russian-troll-tweets/trolls_2016_en.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "troll_sample_df = troll_df.sample(num_samples)\n",
    "troll_sample_df['is_troll'] = 1\n",
    "troll_sample_df.rename(columns={'content' : 'text'}, inplace=True)\n",
    "troll_text_df = troll_sample_df[['text', 'is_troll']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Data and Write to h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.concat([non_troll_text_df, troll_text_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fe905ec75f94716897b26d0f82fa0ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=40000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Write embeddings to h5py file\n",
    "embed_file = './data/embeddings.h5' \n",
    "num_tweets = len(text_df)\n",
    "batch_size = 128\n",
    "pbar = tqdm(total=num_tweets)\n",
    "with h5py.File(embed_file, \"a\") as f:\n",
    "    embed_dset = f.create_dataset('embeddings', (num_tweets, 4096))\n",
    "    response_dset = f.create_dataset('is_troll', (num_tweets,))\n",
    "    counter = 0\n",
    "    for batch in GetBatch(text_df, 'text', 'is_troll', batch_size=batch_size):\n",
    "        embeddings_  = GetEmbeddings(batch[0].values, batch_size=batch_size)\n",
    "        is_troll_ = batch[1].values\n",
    "        num_tweets = batch[0].shape[0]\n",
    "        embed_dset[counter:(counter + num_tweets)] = embeddings_\n",
    "        response_dset[counter:(counter + num_tweets)] = is_troll_\n",
    "        counter += num_tweets\n",
    "        pbar.update(num_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read h5py file\n",
    "with h5py.File(embed_file, \"r\") as f:\n",
    "    keys = list(f.keys())\n",
    "    X = np.array(f[keys[0]])\n",
    "    y = np.array(f[keys[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ShuffleArrays(X, y):\n",
    "    arr = np.arange(X.shape[0])\n",
    "    np.random.shuffle(arr)\n",
    "    X = X[arr,:]\n",
    "    y = y[arr]\n",
    "    return X, y\n",
    "def GetTrainBatches(X, y, batch_size=256):\n",
    "    data_len = X.shape[0]\n",
    "    for counter in range(int(data_len/batch_size)+1):\n",
    "        yield X[counter*batch_size:min((counter + 1)*batch_size, data_len), :], \\\n",
    "        y[counter*batch_size:min((counter + 1)*batch_size, data_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(TwitterNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(4096,1024)\n",
    "        self.drop1 = nn.Dropout()\n",
    "        self.fc2 = nn.Linear(1024,512)\n",
    "        self.fc3 = nn.Linear(512,128)\n",
    "        self.fc4 = nn.Linear(128,num_classes)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = Variable(torch.FloatTensor(X_train))\n",
    "y_train = Variable(torch.FloatTensor(y_train))\n",
    "X_test = Variable(torch.FloatTensor(X_test))\n",
    "y_test = Variable(torch.FloatTensor(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "model_net = TwitterNet(2).to(device)\n",
    "optimizer = SGD(model_net.parameters(), lr = 0.1, momentum=0.9)\n",
    "num_epochs = 15\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy on untrained net is 49.88%\n"
     ]
    }
   ],
   "source": [
    "# Check accuracy on untrained network (with randomly initialized weights)\n",
    "torch.set_grad_enabled(False)\n",
    "running_corrects = 0\n",
    "for inputs, labels in GetTrainBatches(X_test, y_test, batch_size):\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs = model_net(inputs)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    running_corrects += torch.sum(preds == labels.long().data)\n",
    "val_acc = running_corrects.double()/X_test.shape[0]\n",
    "torch.set_grad_enabled(True)\n",
    "print('Validation Accuracy on untrained net is {:.2%}'.format(val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 1 is 68.002. Train Acc. is 74.58% and Validation Acc. is 84.15%\n",
      "Loss after epoch 2 is 45.727. Train Acc. is 85.19% and Validation Acc. is 85.40%\n",
      "Loss after epoch 3 is 42.936. Train Acc. is 86.12% and Validation Acc. is 86.17%\n",
      "Loss after epoch 4 is 41.585. Train Acc. is 86.52% and Validation Acc. is 85.47%\n",
      "Loss after epoch 5 is 41.159. Train Acc. is 86.63% and Validation Acc. is 85.67%\n",
      "Loss after epoch 6 is 40.199. Train Acc. is 87.08% and Validation Acc. is 85.42%\n",
      "Loss after epoch 7 is 40.141. Train Acc. is 86.85% and Validation Acc. is 85.88%\n",
      "Loss after epoch 8 is 40.263. Train Acc. is 86.61% and Validation Acc. is 85.95%\n",
      "Loss after epoch 9 is 39.558. Train Acc. is 87.10% and Validation Acc. is 84.95%\n",
      "Loss after epoch 10 is 38.945. Train Acc. is 87.26% and Validation Acc. is 86.08%\n",
      "Loss after epoch 11 is 38.207. Train Acc. is 87.34% and Validation Acc. is 86.17%\n",
      "Loss after epoch 12 is 38.101. Train Acc. is 87.42% and Validation Acc. is 86.05%\n",
      "Loss after epoch 13 is 38.115. Train Acc. is 87.24% and Validation Acc. is 85.25%\n",
      "Loss after epoch 14 is 38.777. Train Acc. is 87.18% and Validation Acc. is 85.72%\n",
      "Loss after epoch 15 is 38.262. Train Acc. is 87.32% and Validation Acc. is 85.12%\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(True)\n",
    "for epoch in range(num_epochs):\n",
    "    X_train, y_train = ShuffleArrays(X_train, y_train)\n",
    "    running_loss = 0.0\n",
    "    train_corrects = 0\n",
    "    for inputs, labels in GetTrainBatches(X_train, y_train, batch_size):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model_net(inputs)\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        train_corrects += torch.sum(preds == labels.long().data)\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Validation set accuracy\n",
    "    train_acc = train_corrects.double()/X_train.shape[0]\n",
    "    torch.set_grad_enabled(False)\n",
    "    running_corrects = 0\n",
    "    for inputs, labels in GetTrainBatches(X_test, y_test, batch_size):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model_net(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_corrects += torch.sum(preds == labels.long().data)\n",
    "    val_acc = running_corrects.double()/X_test.shape[0]\n",
    "    torch.set_grad_enabled(True)\n",
    "    print('Loss after epoch {} is {:.3f}. Train Acc. is {:.2%} and Validation Acc. is {:.2%}'.\\\n",
    "          format(epoch+1, running_loss, train_acc, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy on untrained net is 85.05%\n"
     ]
    }
   ],
   "source": [
    "# Check accuracy on trained network\n",
    "torch.set_grad_enabled(False)\n",
    "running_corrects = 0\n",
    "for inputs, labels in GetTrainBatches(X_test, y_test, batch_size):\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs = model_net(inputs)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    running_corrects += torch.sum(preds == labels.long().data)\n",
    "val_acc = running_corrects.double()/X_test.shape[0]\n",
    "torch.set_grad_enabled(True)\n",
    "print('Validation Accuracy on untrained net is {:.2%}'.format(val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1995.0"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_test[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
